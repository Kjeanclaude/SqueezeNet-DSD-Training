<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>DSD Training by songhan</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">DSD Training</h1>
      <h2 class="project-tagline">Applying Dense-Sparse-Dense training methodology to SqueezeNet, DSD training improved the top-1 accuracy of SqueezeNet by 4.3% on ImageNet without changing the model architecture and model size. </h2>
      <a href="https://github.com/songhan/SqueezeNet-DSD-Training" class="btn">View on GitHub</a>
      <a href="https://github.com/songhan/SqueezeNet-DSD-Training/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/songhan/SqueezeNet-DSD-Training/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h2>
<a id="what-is-dsd-training" class="anchor" href="#what-is-dsd-training" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>What is DSD Training?</h2>

<p>DSD training is an interesting byproduct of network pruning: re-densifying and retraining from a sparse model can improve the accuracy. That is, compared to a dense CNN baseline, dense→sparse→dense (DSD) training yielded higher accuracy. </p>

<p>The repo contains the DSD-SqueezeNet caffemodel, which is obtained by applying Dense-Sparse-Dense training methodology to SqueezeNet v1.0. DSD training methodology improves the top-1 accuracy of SqueezeNet by 4.3% on ImageNet without changing the model architecture and model size. </p>

<p>We now explain our DSD training strategy. On top of the sparse SqueezeNet (pruned 3x, but same accuracy), we let the killed weights recover, initializing them from zero. We let the survived weights keeping their value. We retrained the whole network using learning rate of 1e − 4. After 20 epochs of training, we observed that the top-1 ImageNet accuracy improved by 4.3 percentage-points; </p>

<h2>
<a id="why-dsd-training-works" class="anchor" href="#why-dsd-training-works" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Why DSD training works?</h2>

<p>Sparsity is a powerful form of regularization. Our intuition is that, once the network arrives at a local minimum given the sparsity constraint, relaxing the constraint gives the network more freedom to escape the saddle point and arrive at a higher-accuracy local minimum. So far, we trained in just three stages of density (dense→sparse→dense), but regularizing models by intermittently pruning parameters10 throughout training would be an interesting area of future work.</p>

<h2>
<a id="usage" class="anchor" href="#usage" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Usage:</h2>

<pre><code>$CAFFE_ROOT/build/tools/caffe test --model=trainval.prototxt --weights=DSD_SqueezeNet_top1_0.617579_top5_0.834742.caffemodel --iterations=1000 --gpu 0
</code></pre>

<h2>
<a id="result" class="anchor" href="#result" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Result:</h2>

<pre><code>I0421 13:58:46.246104  5184 caffe.cpp:293] accuracy_top1 = 0.617579
I0421 13:58:46.246115  5184 caffe.cpp:293] accuracy_top5 = 0.834742
I0421 13:58:46.246126  5184 caffe.cpp:293] loss = 1.7059 (* 1 = 1.7059 loss)    
</code></pre>

<h2>
<a id="related-squeezenet-repo" class="anchor" href="#related-squeezenet-repo" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Related SqueezeNet Repo:</h2>

<p><a href="https://github.com/DeepScale/SqueezeNet">SqueezeNet</a></p>

<p><a href="https://github.com/songhan/SqueezeNet-Deep-Compression">SqueezeNet-Deep-Compression</a></p>

<p><a href="https://github.com/songhan/SqueezeNet-Generator">SqueezeNet-Generator</a></p>

<p><a href="https://github.com/songhan/SqueezeNet-DSD-Training">SqueezeNet-DSD-Training</a></p>

<p><a href="https://github.com/songhan/SqueezeNet-Residual">SqueezeNet-Residual</a></p>

<h2>
<a id="related-papers" class="anchor" href="#related-papers" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Related Papers</h2>

<p><a href="http://arxiv.org/pdf/1602.07360v3.pdf">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5MB model size</a></p>

<p><a href="http://arxiv.org/pdf/1506.02626v3.pdf">Learning both Weights and Connections for Efficient Neural Network (NIPS'15)</a></p>

<p><a href="http://arxiv.org/pdf/1510.00149v5.pdf">Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding (ICLR'16, best paper award)</a></p>

<p><a href="http://arxiv.org/pdf/1602.01528v1.pdf">EIE: Efficient Inference Engine on Compressed Deep Neural Network (ISCA'16)</a></p>

<p>If you find SqueezeNet, DSD training, network pruning and Deep Compression useful in your research, please consider citing the paper:</p>

<pre><code>@article{SqueezeNet,
  title={SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and&lt; 0.5MB model size},
  author={Iandola, Forrest N and Han, Song and Moskewicz, Matthew W and Ashraf, Khalid and Dally, William J and Keutzer, Kurt},
  journal={arXiv preprint arXiv:1602.07360},
  year={2016}
}

@article{DeepCompression,
  title={Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization and Huffman Coding},
  author={Han, Song and Mao, Huizi and Dally, William J},
  journal={International Conference on Learning Representations (ICLR)},
  year={2016}
}

@inproceedings{han2015learning,
  title={Learning both Weights and Connections for Efficient Neural Network},
  author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
  booktitle={Advances in Neural Information Processing Systems (NIPS)},
  pages={1135--1143},
  year={2015}
}

@article{han2016eie,
  title={EIE: Efficient Inference Engine on Compressed Deep Neural Network},
  author={Han, Song and Liu, Xingyu and Mao, Huizi and Pu, Jing and Pedram, Ardavan and Horowitz, Mark A and Dally, William J},
  journal={International Conference on Computer Architecture (ISCA)},
  year={2016}
}
</code></pre>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/songhan/SqueezeNet-DSD-Training">DSD Training</a> is maintained by <a href="https://github.com/songhan">songhan</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

  
  </body>
</html>
